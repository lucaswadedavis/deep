{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clayton (cemellina@gmail.com) is a Machine Learning Expert at Yahoo\n",
    "- Object recognition models\n",
    "- Face recognition for celebrities\n",
    "- Smart Cropping\n",
    "- Aesthetic Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Course\n",
    "- What is a NN?\n",
    "- How do we train a NN?\n",
    "- What is it doing?\n",
    "- What are the parts of a complete model?\n",
    "- Tomorrow: a CNN and an RNN!\n",
    "- Today: an image recognition system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Metanotes about Running Workshops\n",
    "- Jupyter is great\n",
    "- Tons of time is wasted in Environment setup - do everything you can to avoid individual setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Installation Notes\n",
    "- pip install scikit-learn _not_ pip install skit_learn\n",
    "- change the backend from *theano* to *tensorflow* in ~/.keras/keras.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 1\n",
    "- inputs weights summation bias activation function and output\n",
    "- an **Epoch** is a full pass through the training data\n",
    "- **model = data + structure + loss + optimizer** (the master formula)\n",
    "- a **Batch** is a group of inputs that we process at the same time\n",
    "- batch size matters\n",
    "- an **Epoch** can have multiple **Batches**\n",
    "- **optimizers** often set the learning rate (lr)\n",
    "- **Deep Learning** is *Deep* because there are lots of layers\n",
    "- 2 layers make a model *Deep*\n",
    "- parameters: weights and biases (theta)\n",
    "- *Layers* that aren't at the bounds are called **hidden** because we don't know what their values are\n",
    "- Deep models are also cometimes called *Multilayer Perceptrons* (MLP)\n",
    "- all MLPs are _fully connected_\n",
    "- if you can draw a decent line between two \"blobs\" the dataset is said to be *Linearly Separable*\n",
    "- weights are linear - activation functions are non-linear\n",
    "- **RELU** (rectified linear unit) is the most popular activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 2\n",
    "- **Optimization**\n",
    "- *Loss Surfaces* are often highly irregular\n",
    "- *Learning Rate* (Alpha) is the step size or epsilon in your gradient descent algorithm\n",
    "- Learning Rate is more important than Batch Size\n",
    "- If the main knob you can tune is the Learning Rate, then the main plot you care to look at is the Loss Plot\n",
    "- Backprop is an algorithm for efficiently calculating gradients for gradient descent\n",
    "- Backprop is the chain rule in action\n",
    "- Multinomial vs Binomial crossentropy (different kinds of loss functions)\n",
    "- Mean squared error is probably the best loss function for regression type problems\n",
    "- *Validation Data* = Test Data\n",
    "- Deep Learning seems to have a bigger problem with overfitting than other techniques in ML\n",
    "- The 3 best things to improve your models\n",
    "    - Get more Data\n",
    "    - Weight regularization (maybe because it helps you ignore outliers in the dataset - unclear)\n",
    "    - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3\n",
    "- **TensorFlow**\n",
    "- **Broadcasting**: multiplying and adding tensors, matrices, vectors, and scalars\n",
    "- **Graph**: Collections of Tensors and operations on those Tensors (like functions)\n",
    "- **Placeholders**: the *arguments* to a graph (function)\n",
    "- **Variables**: are parameters (like weights and biases - things you can move up and down) but they're not *Placeholders* (arguments)\n",
    "- **Session**: Actually executes the model after construction (maybe important to distributed computation?)\n",
    "- Use Cafe in Production (it's not a good tool to prototype or learn Deep Learning, but it is a good tool to deploy models)\n",
    "- **Convolutional Neural Networks**\n",
    "- *Classification* vs *Detection* (the image contains a cat vs here is the cat)\n",
    "- *Convolve* a filter with an image\n",
    "- Properties of Convolution to think about\n",
    "    - Convolution has shared parameters\n",
    "    - Convolution is a *sparse* operation, not everything connects to everything else\n",
    "    - (it seems to me like convolution is about keeping structure and a kind of micro-scale ensembling)\n",
    "    - Weights are 4 dimensional at every layer\n",
    "    - **Stride** is a measure of how many *pixels* to step when incrementing across the source\n",
    "    - **Padding** surrounding the inputs with zeros to handle strides that are too large to evenly fit in the filters\n",
    "    - Not *Padding* is called **Same Padding**\n",
    "    - The number of *filters* you use in a CNN roughly corresponds to teh number of layers in an MLP\n",
    "    - A **Pooling** Layer also has a size (like a Convolutional layer - say 2x2) and it works like a reducer a Pooling Layer may output max for example or average\n",
    "    - Pooling can be used to decrease the ouput dimensionality\n",
    "    - A **Flatten** Layer will taken an input tensor and \"unroll it\" to make a vector\n",
    "    - It's very common to use several Convolutional layers, then to flatten it out to hand it off to softmax for output, that pattern is what is usually referred to as a CNN\n",
    "- Keras has some terminology duplication\n",
    "    - border mode = padding\n",
    "    - filter = kernal\n",
    "    - stride = subsample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 4\n",
    "- Feature Maps\n",
    "- **tSNE** a non-linear dimensionality reduction technique\n",
    "- *PCA* (Principle Component Analysis) is a linear dimensionality reduction technique\n",
    "- **RNNs**\n",
    "- make good sequential classifiers\n",
    "- Text is a great example - it's all sequential\n",
    "- Timeseries another\n",
    "- One Hot encode vocabulary.\n",
    "- This is also called embedding\n",
    "- RNN's have a hard time remembering, the problem of **Long-Term Dependencies**\n",
    "- **LSTMs** (Long Short-Term Memory)\n",
    "- *Element-Wise* per element application of an activation function (the sigmoid activation function) to each element in a vector\n",
    "- **Forget Gates** allow models to throw away information they don't think they need anymore\n",
    "- These things commonly use *Tanh* activation functions\n",
    "- Future Learning\n",
    "    - Do projects, Play\n",
    "    - Find a friend\n",
    "    - Take advantage of resources\n",
    "        - Keras has a great set of examples\n",
    "        - Keras documentation has lots of links to good papers\n",
    "        - There's a Udacity course for TensorFlow that's pretty good\n",
    "        - Stanford course on Deep Learning for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "- [TensorFlow Playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.73305&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false)\n",
    "- [Activation Functions](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)\n",
    "- [ICML Papers](http://icml.cc/2016/?page_id=1649)\n",
    "- [Tensor Board](https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Notes\n",
    "- There's no real complexity here, it's all just jargon\n",
    "- Just because you can use equations to represent something, doesn't mean that equations are the best way to represent it. Such is Machine Learning...\n",
    "- Though, writing down an equation with Greek letters is a good way to intimidate people\n",
    "- I now think I understand everything from the Party last Saturday\n",
    "- I can use these Jupyter notebooks with Morgan\n",
    "- Anyone can do this\n",
    "- Science is hard, Machine Learning is easy\n",
    "- It kind of seems like ML libraries were created by mathematicians, not engineers. They're recapitulating a ton of stuff, which in practice looks a lot like needlessly renaming things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
