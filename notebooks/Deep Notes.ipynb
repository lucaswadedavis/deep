{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clayton is a Machine Learning Expert at Yahoo\n",
    "- Object recognition models\n",
    "- Face recognition for celebrities\n",
    "- Smart Cropping\n",
    "- Aesthetic Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Course\n",
    "- What is a NN?\n",
    "- How do we train a NN?\n",
    "- What is it doing?\n",
    "- What are the parts of a complete model?\n",
    "- Tomorrow: a CNN and an RNN!\n",
    "- Today: an image recognition system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Metanotes about Running Workshops\n",
    "- Jupyter is great\n",
    "- Tons of time is wasted in Environment setup - do everything you can to avoid individual setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Installation Notes\n",
    "- pip install scikit-learn _not_ pip install skit_learn\n",
    "- change the backend from *theano* to *tensorflow* in ~/.keras/keras.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 1\n",
    "- inputs weights summation bias activation function and output\n",
    "- an **Epoch** is a full pass through the training data\n",
    "- **model = data + structure + loss + optimizer** (the master formula)\n",
    "- a **Batch** is a group of inputs that we process at the same time\n",
    "- batch size matters\n",
    "- an **Epoch** can have multiple **Batches**\n",
    "- **optimizers** often set the learning rate (lr)\n",
    "- **Deep Learning** is *Deep* because there are lots of layers\n",
    "- 2 layers make a model *Deep*\n",
    "- parameters: weights and biases (theta)\n",
    "- *Layers* that aren't at the bounds are called **hidden** because we don't know what their values are\n",
    "- Deep models are also cometimes called *Multilayer Perceptrons* (MLP)\n",
    "- all MLPs are _fully connected_\n",
    "- if you can draw a decent line between two \"blobs\" the dataset is said to be *Linearly Separable*\n",
    "- weights are linear - activation functions are non-linear\n",
    "- **RELU** (rectified linear unit) is the most popular activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 2\n",
    "- **Optimization**\n",
    "- *Loss Surfaces* are often highly irregular\n",
    "- *Learning Rate* (Alpha) is the step size or epsilon in your gradient descent algorithm\n",
    "- Learning Rate is more important than Batch Size\n",
    "- If the main knob you can tune is the Learning Rate, then the main plot you care to look at is the Loss Plot\n",
    "- Backprop is an algorithm for efficiently calculating gradients for gradient descent\n",
    "- Backprop is the chain rule in action\n",
    "- Multinomial vs Binomial crossentropy (different kinds of loss functions)\n",
    "- Mean squared error is probably the best loss function for regression type problems\n",
    "- *Validation Data* = Test Data\n",
    "- Deep Learning seems to have a bigger problem with overfitting than other techniques in ML\n",
    "- The 3 best things to improve your models\n",
    "    - Get more Data\n",
    "    - Weight regularization (maybe because it helps you ignore outliers in the dataset - unclear)\n",
    "    - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3\n",
    "- **TensorFlow**\n",
    "- **Broadcasting**: multiplying and adding tensors, matrices, vectors, and scalars\n",
    "- **Graph**: Collections of Tensors and operations on those Tensors (like functions)\n",
    "- **Placeholders**: the *arguments* to a graph (function)\n",
    "- **Variables**: are parameters (like weights and biases - things you can move up and down) but they're not *Placeholders* (arguments)\n",
    "- **Session**: Actually executes the model after construction (maybe important to distributed computation?)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "- [TensorFlow Playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.73305&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false)\n",
    "- [Activation Functions](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)\n",
    "- [ICML Papers](http://icml.cc/2016/?page_id=1649)\n",
    "- [Tensor Board](https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Notes\n",
    "- There's no real complexity here, it's all just jargon\n",
    "- Just because you can use equations to represent something, doesn't mean that equations are the best way to represent it. Such is Machine Learning...\n",
    "- Though, writing down an equation with Greek letters is a good way to intimidate people\n",
    "- I now think I understand everything from the Party last Saturday\n",
    "- I can use these Jupyter notebooks with Morgan\n",
    "- Anyone can do this\n",
    "- Science is hard, Machine Learning is easy\n",
    "- It kind of seems like ML libraries were created by mathematicians, not engineers. They're recapitulating a ton of stuff, which in practice looks a lot like needlessly renaming things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
